{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287e812a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!unzip \"/content/Non cattle (2).zip\" -d \"/content/\" # Should show your 5 breed folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1265442",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Final Breed Classification by Pashuvision\n",
    "\"\"\"\n",
    "train_breed_final.py\n",
    "Same pipeline as #final trial, but with post-hoc calibration + prob averaging\n",
    " higher confidence, identical accuracy.\n",
    "\"\"\"\n",
    "import os, json, time, torch, torchvision, numpy as np, shutil, pandas as pd\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, models\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torch.amp import autocast, GradScaler\n",
    "from timm.data.auto_augment import rand_augment_transform\n",
    "from timm.data.mixup import Mixup\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "train_dir      = \"/content/Final Dataset/train\"\n",
    "val_dir        = \"/content/Final Dataset/val\"\n",
    "UNLABELLED_DIR = \"/content/unlabelled_images\"\n",
    "PSEUDO_CSV     = \"/content/pseudo_labels.csv\"\n",
    "SAVE_BEST      = \"/content/best_model.pth\"\n",
    "SAVE_FINAL     = \"/content/breed_model_efficientnet_finetuned.pth\"\n",
    "SAVE_LABELS    = \"/content/breed_names.json\"\n",
    "CAL_TEMP_FILE  = \"/content/cal_temp.json\"\n",
    "\n",
    "SOFT_THR       = 0.90\n",
    "TEMP           = 1.0          # will be overwritten by calibrated value\n",
    "RETRAIN_EPOCHS = 5\n",
    "mean, std      = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "\n",
    "# ---------- transforms ----------\n",
    "rand_aug = rand_augment_transform('rand-m7-mstd0.5', {})\n",
    "train_tf = T.Compose([T.Resize((224, 224)), rand_aug, T.ToTensor(), T.Normalize(mean, std)])\n",
    "unlabelled_tf = T.Compose([\n",
    "    T.Resize(342), T.FiveCrop(256),\n",
    "    T.Lambda(lambda crops: torch.stack([T.ToTensor()(c) for c in crops])),\n",
    "    T.Normalize(mean, std)\n",
    "])\n",
    "val_tf = unlabelled_tf        # same as before\n",
    "\n",
    "# ---------- datasets ----------\n",
    "train_ds = datasets.ImageFolder(train_dir, transform=train_tf)\n",
    "val_ds   = datasets.ImageFolder(val_dir,   transform=val_tf)\n",
    "breed_names = train_ds.classes\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=4, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# ---------- mixup ----------\n",
    "mixup_fn = Mixup(mixup_alpha=0.1, cutmix_alpha=0.2, prob=1.0,\n",
    "                 num_classes=len(breed_names), label_smoothing=0.05)\n",
    "\n",
    "# ---------- model ----------\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, in_features, n_classes, drop=0.2):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.fc   = nn.Linear(in_features, n_classes)\n",
    "        self.eval_drop = True\n",
    "    def forward(self, x):\n",
    "        if self.eval_drop or self.training:\n",
    "            x = self.drop(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "model = models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
    "model.classifier = Head(model.classifier[1].in_features, len(breed_names))\n",
    "model = model.to(device)\n",
    "\n",
    "# ---------- freeze ----------\n",
    "for p in model.parameters(): p.requires_grad = False\n",
    "for p in model.classifier.parameters(): p.requires_grad = True\n",
    "\n",
    "# ---------- class weights ----------\n",
    "labels = train_ds.targets\n",
    "weights = torch.tensor(\n",
    "    compute_class_weight('balanced', classes=np.arange(len(breed_names)), y=labels),\n",
    "    dtype=torch.float, device=device)\n",
    "\n",
    "# ---------- optim ----------\n",
    "optimizer = optim.AdamW(model.classifier.parameters(), lr=3e-3, weight_decay=5e-3)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer, T_0=len(train_loader)*5, T_mult=1, eta_min=1e-6)\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def unfreeze_and_add(stage, lr):\n",
    "    frozen = []\n",
    "    for name, module in model.features.named_children():\n",
    "        if int(name) >= stage:\n",
    "            for p in module.parameters():\n",
    "                if not p.requires_grad:\n",
    "                    p.requires_grad = True\n",
    "                    frozen.append(p)\n",
    "    if frozen:\n",
    "        optimizer.add_param_group({'params': frozen, 'lr': lr})\n",
    "\n",
    "# ---------- global metrics ----------\n",
    "best_val_loss = 1e9\n",
    "patience = 15\n",
    "pat_counter = 0\n",
    "accum = 2\n",
    "grad_clip = 1.0\n",
    "\n",
    "\n",
    "#  1. MAIN TRAINING  (unchanged)\n",
    "def main_training():\n",
    "    global best_val_loss, pat_counter\n",
    "    for epoch in range(30):\n",
    "        print(f\"\\n-----  Epoch {epoch+1}/30 -----\")\n",
    "        if epoch == 3: unfreeze_and_add(6, 3e-4)\n",
    "        if epoch == 6: unfreeze_and_add(4, 1e-4)\n",
    "        if epoch == 9: unfreeze_and_add(0, 5e-5)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights if epoch < 10 else None, label_smoothing=0.05)\n",
    "\n",
    "        model.train()\n",
    "        running = 0.\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            x, y = mixup_fn(x, y)\n",
    "            with autocast('cuda'):\n",
    "                out = model(x)\n",
    "                loss = criterion(out, y) / accum\n",
    "            scaler.scale(loss).backward()\n",
    "            if (i+1) % accum == 0 or i+1 == len(train_loader):\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                scaler.step(optimizer); scaler.update(); optimizer.zero_grad(set_to_none=True)\n",
    "            running += loss.item() * accum\n",
    "        print(f\" Train Loss: {running/len(train_loader):.4f}\")\n",
    "\n",
    "        # ---- validate ----\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0., 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                B = y.size(0)\n",
    "                x = x.view(-1, 3, 256, 256).to(device)\n",
    "                y = y.to(device)\n",
    "                with autocast('cuda'):\n",
    "                    # NEW: average probabilities, not logits\n",
    "                    prob = torch.softmax(model(x), 1).view(5, B, -1).mean(0)\n",
    "                    prob_flip = torch.softmax(model(torch.flip(x, dims=[-1])), 1).view(5, B, -1).mean(0)\n",
    "                    prob = (prob + prob_flip) / 2\n",
    "                    val_loss += nn.CrossEntropyLoss()(torch.log(prob + 1e-8), y).item()\n",
    "                pred = prob.argmax(1)\n",
    "                correct += (pred == y).sum().item()\n",
    "                total += B\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100 * correct / total\n",
    "        print(f\" Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        with open(SAVE_BEST.replace(\".pth\", \"_val_acc.txt\"), \"w\") as f:\n",
    "            f.write(f\"{val_acc:.2f}%\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), SAVE_BEST)\n",
    "            pat_counter = 0\n",
    "            print(f\" New best model saved (val_loss={val_loss:.4f})\")\n",
    "        else:\n",
    "            pat_counter += 1\n",
    "            if pat_counter >= patience:\n",
    "                print(\" Early stopping triggered\")\n",
    "                break\n",
    "        scheduler.step()\n",
    "\n",
    "    model.load_state_dict(torch.load(SAVE_BEST))\n",
    "    torch.save(model, SAVE_FINAL)\n",
    "    with open(SAVE_LABELS, \"w\") as f:\n",
    "        json.dump(breed_names, f)\n",
    "    print(\" Main training finished – best model saved\")\n",
    "\n",
    "#  2. TEMPERATURE CALIBRATION (new)\n",
    "def calibrate_temperature():\n",
    "    model.eval()\n",
    "    logits_list, labels_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            B = y.size(0)\n",
    "            x = x.view(-1, 3, 256, 256).to(device)\n",
    "            y = y.to(device)\n",
    "            prob = torch.softmax(model(x), 1).view(5, B, -1).mean(0)\n",
    "            prob_flip = torch.softmax(model(torch.flip(x, dims=[-1])), 1).view(5, B, -1).mean(0)\n",
    "            prob = (prob + prob_flip) / 2\n",
    "            logit = torch.log(prob + 1e-8)            # calibrated logit\n",
    "            logits_list.append(logit)\n",
    "            labels_list.append(y)\n",
    "    logits = torch.cat(logits_list)   # [N, C]\n",
    "    labels = torch.cat(labels_list)   # [N]\n",
    "\n",
    "    def ece(p, y, n_bins=15):\n",
    "        bin_boundaries = torch.linspace(0,1,n_bins+1)\n",
    "        bin_lowers = bin_boundaries[:-1]\n",
    "        bin_uppers = bin_boundaries[1:]\n",
    "        conf, pred = p.max(1)\n",
    "        acc = pred.eq(y).float()\n",
    "        ece = 0.\n",
    "        for bl, bu in zip(bin_lowers, bin_uppers):\n",
    "            in_bin = conf.gt(bl) * conf.le(bu)\n",
    "            prop = in_bin.float().mean()\n",
    "            if prop > 0:\n",
    "                acc_bin = acc[in_bin].mean()\n",
    "                conf_bin = conf[in_bin].mean()\n",
    "                ece += torch.abs(acc_bin - conf_bin) * prop\n",
    "        return ece\n",
    "\n",
    "    def opt_temp(logits, labels, t_min=0.2, t_max=3.0, steps=50):\n",
    "        best_t, best_ece = 1.0, 1e6\n",
    "        for t in torch.linspace(t_min, t_max, steps):\n",
    "            p = torch.softmax(logits/t, 1)\n",
    "            e = ece(p, labels)\n",
    "            if e < best_ece:\n",
    "                best_ece, best_t = e, t.item()\n",
    "        return best_t\n",
    "\n",
    "    T_cal = opt_temp(logits, labels)\n",
    "    json.dump({'T': T_cal}, open(CAL_TEMP_FILE, 'w'))\n",
    "    print(f' Calibrated temperature = {T_cal:.3f}')\n",
    "\n",
    "\n",
    "#  3. PSEUDO + HUMAN-IN-THE-LOOP  (confidence fix inside)\n",
    "def predict_image(image_path, temp=None):\n",
    "    if temp is None:\n",
    "        if os.path.exists(CAL_TEMP_FILE):\n",
    "            temp = json.load(open(CAL_TEMP_FILE))['T']\n",
    "        else:\n",
    "            temp = 1.0\n",
    "    model.eval()\n",
    "    model.classifier.eval_drop = False   # <- deterministic\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    crops = unlabelled_tf(img).to(device)\n",
    "    with torch.no_grad():\n",
    "        with autocast('cuda'):\n",
    "            # average probabilities\n",
    "            prob = torch.softmax(model(crops) / temp, 1).mean(0)\n",
    "            prob_flip = torch.softmax(model(torch.flip(crops, dims=[-1])) / temp, 1).mean(0)\n",
    "            prob = (prob + prob_flip) / 2\n",
    "    top3 = torch.topk(prob, k=3)\n",
    "    return [(breed_names[i], float(p)) for i, p in zip(top3.indices, top3.values)], prob.cpu()\n",
    "\n",
    "def worker_choice(image_path, top3):\n",
    "    print(f\"\\n Image: {image_path}\")\n",
    "    for idx, (b, p) in enumerate(top3, 1):\n",
    "        print(f\"  {idx}. {b}  ({p*100:.1f}%)\")\n",
    "    while True:\n",
    "        try:\n",
    "            choice = input(\" Choose 1-3 (or 0 to skip): \").strip()\n",
    "            if choice == \"0\":\n",
    "                return None\n",
    "            choice = int(choice)\n",
    "            if 1 <= choice <= 3:\n",
    "                return top3[choice-1][0]\n",
    "            else:\n",
    "                print(\" Please enter 0, 1, 2, or 3.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input – please enter a number.\")\n",
    "\n",
    "def create_pseudo_csv():\n",
    "    unlabelled_path = Path(UNLABELLED_DIR)\n",
    "    if not unlabelled_path.exists():\n",
    "        print(f\" Unlabelled directory not found: {UNLABELLED_DIR}\")\n",
    "        pd.DataFrame(columns=['file', 'label', 'confidence', 'type']).to_csv(PSEUDO_CSV, index=False)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    image_files = list(unlabelled_path.rglob(\"*.[jJ][pP][gG]\")) + \\\n",
    "                  list(unlabelled_path.rglob(\"*.[jJ][pP][eE][gG]\")) + \\\n",
    "                  list(unlabelled_path.rglob(\"*.[pP][nN][gG]\"))\n",
    "\n",
    "    if len(image_files) == 0:\n",
    "        print(f\" No images found in {UNLABELLED_DIR}\")\n",
    "        pd.DataFrame(columns=['file', 'label', 'confidence', 'type']).to_csv(PSEUDO_CSV, index=False)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\" Found {len(image_files)} images to label...\")\n",
    "    records = []\n",
    "\n",
    "    existing_files = set()\n",
    "    if os.path.exists(PSEUDO_CSV):\n",
    "        try:\n",
    "            existing_df = pd.read_csv(PSEUDO_CSV)\n",
    "            existing_files = set(existing_df['file'].tolist())\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for img_path in image_files:\n",
    "        if str(img_path) in existing_files:\n",
    "            print(f\"⏭ Already labeled: {img_path.name}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            top3, soft_prob = predict_image(img_path)\n",
    "            best_breed, best_conf = top3[0]\n",
    "\n",
    "            if best_conf >= SOFT_THR:\n",
    "                records.append({\n",
    "                    'file': str(img_path),\n",
    "                    'label': best_breed,\n",
    "                    'confidence': best_conf,\n",
    "                    'type': 'pseudo'\n",
    "                })\n",
    "                print(f\" Auto-labeled: {img_path.name} → {best_breed} ({best_conf:.2f})\")\n",
    "            else:\n",
    "                chosen = worker_choice(img_path, top3)\n",
    "                if chosen is not None:\n",
    "                    records.append({\n",
    "                        'file': str(img_path),\n",
    "                        'label': chosen,\n",
    "                        'confidence': 1.0,\n",
    "                        'type': 'human'\n",
    "                    })\n",
    "                    print(f\" Human-labeled: {img_path.name} → {chosen}\")\n",
    "                else:\n",
    "                    print(f\" Skipped: {img_path.name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error processing {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if os.path.exists(PSEUDO_CSV) and len(records) > 0:\n",
    "        try:\n",
    "            existing_df = pd.read_csv(PSEUDO_CSV)\n",
    "            new_df = pd.DataFrame(records)\n",
    "            df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "        except:\n",
    "            df = pd.DataFrame(records)\n",
    "    else:\n",
    "        df = pd.DataFrame(records)\n",
    "\n",
    "    df.to_csv(PSEUDO_CSV, index=False)\n",
    "    print(f\" CSV saved → {len(df)} total labels collected\")\n",
    "    return df\n",
    "\n",
    "\n",
    "#  4. SOFT-HARD DATASET + RETRAIN  (unchanged)\n",
    "class SoftHardDataset(Dataset):\n",
    "    def __init__(self, csv_file, root, transform, class_to_idx):\n",
    "        if not os.path.exists(csv_file):\n",
    "            self.samples = []\n",
    "            return\n",
    "        try:\n",
    "            self.df = pd.read_csv(csv_file)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            self.df = pd.DataFrame()\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.num_classes = len(class_to_idx)\n",
    "        self.samples = []\n",
    "        if len(self.df) > 0:\n",
    "            for _, row in self.df.iterrows():\n",
    "                self.samples.append((row['file'], row['label'], row['confidence'], row['type']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label, conf, typ = self.samples[idx]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # >>> FIX: Always return a tensor of shape [num_classes] <<<\n",
    "        target = torch.zeros(self.num_classes)\n",
    "\n",
    "        label_idx = self.class_to_idx[label]\n",
    "\n",
    "        if typ == 'pseudo':\n",
    "            target[label_idx] = conf\n",
    "        else:  # human\n",
    "            target[label_idx] = 1.0  # One-hot encoding\n",
    "\n",
    "        return img, target.float()\n",
    "\n",
    "def retrain_with_new_data(extra_epochs=RETRAIN_EPOCHS):\n",
    "    global accum, grad_clip\n",
    "\n",
    "    print(\"\\n Checking for pseudo-labels...\")\n",
    "\n",
    "    # Safety check: if CSV doesn't exist or is empty, skip\n",
    "    if not os.path.exists(PSEUDO_CSV):\n",
    "        print(\" Pseudo-labels CSV not found — skipping retraining.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(PSEUDO_CSV)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"Pseudo-labels CSV is empty — skipping retraining.\")\n",
    "        return\n",
    "\n",
    "    if len(df) == 0:\n",
    "        print(\" No pseudo-labels collected — skipping retraining.\")\n",
    "        return\n",
    "\n",
    "    print(f\" Found {len(df)} pseudo/human labels — starting retraining...\")\n",
    "\n",
    "    merged_root = \"/content/merged_train\"\n",
    "    os.makedirs(merged_root, exist_ok=True)\n",
    "\n",
    "    # Copy original training data\n",
    "    os.system(f\"cp -r {train_dir}/* {merged_root}/ 2>/dev/null || echo 'Original data copied'\")\n",
    "\n",
    "    # Copy pseudo-labeled images into breed folders\n",
    "    for _, row in df.iterrows():\n",
    "        src = Path(row['file'])\n",
    "        breed = row['label']\n",
    "        breed_dir = Path(merged_root) / breed\n",
    "        breed_dir.mkdir(exist_ok=True)\n",
    "        dst = breed_dir / src.name\n",
    "        if not dst.exists():\n",
    "            shutil.copy(src, dst)\n",
    "\n",
    "    # Create dataset and loader\n",
    "    merged_ds = SoftHardDataset(PSEUDO_CSV, merged_root, transform=train_tf,\n",
    "                                class_to_idx=train_ds.class_to_idx)\n",
    "    if len(merged_ds) == 0:\n",
    "        print(\" Merged dataset is empty — skipping retraining.\")\n",
    "        return\n",
    "\n",
    "    merged_loader = DataLoader(merged_ds, batch_size=16, shuffle=True,\n",
    "                               num_workers=2, pin_memory=True)\n",
    "\n",
    "    # FIX: Create FRESH optimizer (don't add_param_group)\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': model.classifier.parameters(), 'lr': 1e-4},\n",
    "        {'params': model.features.parameters(), 'lr': 1e-5}  # Lower LR for backbone\n",
    "    ], weight_decay=5e-3)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                    optimizer, T_0=len(merged_loader)*3, T_mult=1, eta_min=1e-6)\n",
    "\n",
    "    # Custom criterion for soft/hard labels\n",
    "    def criterion(out, y):\n",
    "        if y.dtype is torch.float32:\n",
    "            return -torch.sum(y * torch.log_softmax(out, dim=1), dim=1).mean()\n",
    "        else:\n",
    "            return nn.CrossEntropyLoss()(out, y)\n",
    "\n",
    "    # Retrain loop\n",
    "    for epoch in range(extra_epochs):\n",
    "        print(f\"\\n+++ Pseudo Epoch {epoch+1}/{extra_epochs} +++\")\n",
    "        model.train()\n",
    "        running = 0.\n",
    "        for x, y in merged_loader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            with autocast('cuda'):\n",
    "                out = model(x)\n",
    "                loss = criterion(out, y) / accum\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            scaler.step(optimizer); scaler.update(); optimizer.zero_grad(set_to_none=True)\n",
    "            running += loss.item() * accum\n",
    "        print(f\"Merged Loss: {running/len(merged_loader):.4f}\")\n",
    "\n",
    "    torch.save(model, \"/content/model_after_pseudo.pth\")\n",
    "    print(\" Pseudo-training finished – model saved\")\n",
    "\n",
    "    def criterion(out, y):\n",
    "        if y.dtype is torch.float32:\n",
    "            return -torch.sum(y * torch.log_softmax(out, dim=1), dim=1).mean()\n",
    "        else:\n",
    "            return nn.CrossEntropyLoss()(out, y)\n",
    "\n",
    "    for epoch in range(extra_epochs):\n",
    "        print(f\"\\n+++  Pseudo Epoch {epoch+1}/{extra_epochs} +++\")\n",
    "        model.train()\n",
    "        running = 0.\n",
    "        for x, y in merged_loader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            with autocast('cuda'):\n",
    "                out = model(x)\n",
    "                loss = criterion(out, y) / accum\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            scaler.step(optimizer); scaler.update(); optimizer.zero_grad(set_to_none=True)\n",
    "            running += loss.item() * accum\n",
    "        print(f\" Merged Loss: {running/len(merged_loader):.4f}\")\n",
    "\n",
    "    torch.save(model, \"/content/model_after_pseudo.pth\")\n",
    "    print(\"Pseudo-training finished – model saved\")\n",
    "#5. PRODUCTION: PREDICT + AUTO-LABEL + HUMAN FEEDBACK\n",
    "def predict_and_auto_label(image_path, conf_threshold=0.78):\n",
    "    print(f\"\\n🔍 PRODUCTION MODE: Analyzing {image_path}\")\n",
    "    global model, breed_names\n",
    "    if 'model' not in globals() or model is None:\n",
    "        print(\"Loading model...\")\n",
    "        model = torch.load(SAVE_FINAL)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        model.classifier.eval_drop = False\n",
    "        with open(SAVE_LABELS, \"r\") as f:\n",
    "            breed_names = json.load(f)\n",
    "        print(\" Model loaded.\")\n",
    "\n",
    "    val_acc = \"N/A\"\n",
    "    val_acc_file = SAVE_BEST.replace(\".pth\", \"_val_acc.txt\")\n",
    "    if os.path.exists(val_acc_file):\n",
    "        try:\n",
    "            with open(val_acc_file, \"r\") as f:\n",
    "                val_acc = f.read().strip()\n",
    "        except: pass\n",
    "\n",
    "    top3, prob = predict_image(image_path)\n",
    "    best_breed, best_conf = top3[0]\n",
    "\n",
    "    print(f\" TOP PREDICTION: {best_breed} ({best_conf:.4f})\")\n",
    "    print(f\" LAST KNOWN VAL ACCURACY: {val_acc}\")\n",
    "    print(\"\\n TOP 3 BREEDS:\")\n",
    "    for idx, (breed, conf) in enumerate(top3, 1):\n",
    "        print(f\"  {idx}. {breed} ({conf:.4f})\")\n",
    "\n",
    "    chosen_label = None\n",
    "\n",
    "    if best_conf >= conf_threshold:\n",
    "        print(f\"\\n AUTO-LABELING (Confidence {best_conf:.4f} >= {conf_threshold})\")\n",
    "        chosen_label = best_breed\n",
    "    else:\n",
    "        print(f\"\\n LOW CONFIDENCE ({best_conf:.4f} < {conf_threshold}) — PLEASE CHOOSE:\")\n",
    "        print(\"  1-3: Select from top predictions above\")\n",
    "        print(\"  4. Other (type breed name)\")\n",
    "        print(\"  0. Skip (do not label)\")\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                choice = input(\"\\n Your choice (0-4): \").strip()\n",
    "                if choice == \"0\":\n",
    "                    print(\"⏭ Skipped — no label saved.\")\n",
    "                    return None, None\n",
    "                elif choice in [\"1\", \"2\", \"3\"]:\n",
    "                    chosen_label = top3[int(choice) - 1][0]\n",
    "                    print(f\" You chose: {chosen_label}\")\n",
    "                    break\n",
    "                elif choice == \"4\":\n",
    "                    while True:\n",
    "                        custom_breed = input(\" Enter breed name: \").strip()\n",
    "                        if custom_breed in breed_names:\n",
    "                            chosen_label = custom_breed\n",
    "                            print(f\" Valid breed: {chosen_label}\")\n",
    "                            break\n",
    "                        else:\n",
    "                            print(f\" '{custom_breed}' not in known breeds: {breed_names}\")\n",
    "                            retry = input(\"Try again? (y/n): \").strip().lower()\n",
    "                            if retry != 'y':\n",
    "                                print(\"⏭ Skipped — no label saved.\")\n",
    "                                return None, None\n",
    "                    break\n",
    "                else:\n",
    "                    print(\" Please enter 0, 1, 2, 3, or 4.\")\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n⏭ Interrupted — no label saved.\")\n",
    "                return None, None\n",
    "            except Exception as e:\n",
    "                print(f\" Error: {e} — try again.\")\n",
    "\n",
    "    if chosen_label is not None:\n",
    "        Path(UNLABELLED_DIR).mkdir(parents=True, exist_ok=True)\n",
    "        src_path = Path(image_path)\n",
    "        dest_name = src_path.name\n",
    "        dest_path = Path(UNLABELLED_DIR) / dest_name\n",
    "        counter = 1\n",
    "        while dest_path.exists():\n",
    "            dest_name = f\"{src_path.stem}_{counter}{src_path.suffix}\"\n",
    "            dest_path = Path(UNLABELLED_DIR) / dest_name\n",
    "            counter += 1\n",
    "\n",
    "        shutil.copy(image_path, dest_path)\n",
    "        print(f\" Saved to: {dest_path}\")\n",
    "\n",
    "        label_type = \"pseudo\" if best_conf >= conf_threshold else \"human\"\n",
    "        record = {\n",
    "            'file': str(dest_path),\n",
    "            'label': chosen_label,\n",
    "            'confidence': best_conf if label_type == \"pseudo\" else 1.0,\n",
    "            'type': label_type\n",
    "        }\n",
    "\n",
    "        if os.path.exists(PSEUDO_CSV):\n",
    "            try:\n",
    "                df = pd.read_csv(PSEUDO_CSV)\n",
    "                new_df = pd.DataFrame([record])\n",
    "                df = pd.concat([df, new_df], ignore_index=True)\n",
    "            except:\n",
    "                df = pd.DataFrame([record])\n",
    "        else:\n",
    "            df = pd.DataFrame([record])\n",
    "\n",
    "        df.to_csv(PSEUDO_CSV, index=False)\n",
    "        print(f\" Added to {PSEUDO_CSV} → Ready for next retraining!\")\n",
    "        return chosen_label, best_conf\n",
    "\n",
    "    return None, None\n",
    "\n",
    "\n",
    "#  6. RUN PIPELINE\n",
    "if __name__ == \"__main__\":\n",
    "    print(\" Starting full training pipeline...\")\n",
    "    main_training()\n",
    "    calibrate_temperature()\n",
    "    df = create_pseudo_csv()\n",
    "    if len(df) > 0:\n",
    "        retrain_with_new_data()\n",
    "    else:\n",
    "        print(\" Skipping retraining — no new labels collected.\")\n",
    "\n",
    "    print(\"\\n PIPELINE COMPLETE!\")\n",
    "    print(\" Final model: /content/breed_model_efficientnet_finetuned.pth\")\n",
    "    print(\"  Breed names: /content/breed_names.json\")\n",
    "    if os.path.exists(\"/content/model_after_pseudo.pth\"):\n",
    "        print(\" Upgraded model: /content/model_after_pseudo.pth\")\n",
    "\n",
    "    print(\"\\n TIP: To label NEW images later, run:\")\n",
    "    print(\"   df = label_new_images_only()\")\n",
    "    print(\"   if len(df) > 0:\")\n",
    "    print(\"       retrain_with_new_data()\")\n",
    "\n",
    "    print(\"\\n PRODUCTION TIP: To predict + auto-label/human-label a new image, run:\")\n",
    "    print(\"   breed, conf = predict_and_auto_label('path/to/your/image.jpg', conf_threshold=0.92)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7302fb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "predict_and_auto_label(\"/content/Non cattle/Screenshot 2025-09-20 161316.png\", 0.78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23089383",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "retrain_with_new_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305d5504",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86524f2f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f9c68e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc0798c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a1aeff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101b2af8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7445e94",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# demo_images\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "# Get all validation images, grouped by breed\n",
    "breed_images = {}\n",
    "val_path = Path(\"/content/Final Dataset/val\")\n",
    "\n",
    "for breed_dir in val_path.iterdir():\n",
    "    if breed_dir.is_dir():\n",
    "        # Get all JPG/JPEG/PNG in this breed folder\n",
    "        images = list(breed_dir.rglob(\"*.[jJ][pP][gG]\")) + \\\n",
    "                 list(breed_dir.rglob(\"*.[jJ][pP][eE][gG]\")) + \\\n",
    "                 list(breed_dir.rglob(\"*.[pP][nN][gG]\"))\n",
    "        if images:\n",
    "            breed_images[breed_dir.name] = images\n",
    "\n",
    "print(f\"Found {len(breed_images)} breeds: {list(breed_images.keys())}\")\n",
    "\n",
    "# Shuffle images within each breed\n",
    "for breed in breed_images:\n",
    "    random.shuffle(breed_images[breed])\n",
    "\n",
    "# Create demo folder\n",
    "demo_dir = Path(\"/content/demo_images\")\n",
    "demo_dir.mkdir(exist_ok=True)\n",
    "\n",
    "demo_saved = 0\n",
    "max_per_breed = 2  # Max 2 images per breed → ensures diversity\n",
    "breed_count = {breed: 0 for breed in breed_images}\n",
    "\n",
    "print(\"\\n Building diverse demo set (confidence 90-98%)...\")\n",
    "\n",
    "# Round-robin sampling from each breed\n",
    "while demo_saved < 10 and any(breed_images.values()):\n",
    "    for breed in list(breed_images.keys()):\n",
    "        if breed_count[breed] >= max_per_breed:\n",
    "            continue  # Skip if we already have enough from this breed\n",
    "        if not breed_images[breed]:\n",
    "            continue  # Skip if no more images in this breed\n",
    "\n",
    "        # Take one image from this breed\n",
    "        img_path = breed_images[breed].pop()\n",
    "        try:\n",
    "            top3, prob = predict_image(str(img_path))\n",
    "            best_breed, best_conf = top3[0]\n",
    "            true_breed = breed  # Since it's from this breed's folder\n",
    "\n",
    "            # >>> ONLY ADD IF CONFIDENCE BETWEEN 90% AND 98% AND CORRECT <<<\n",
    "            if 0.90 <= best_conf <= 0.98 and best_breed == true_breed:\n",
    "                # Avoid filename conflicts\n",
    "                dest_name = f\"{breed}_{demo_saved+1}_{int(best_conf*100)}pct{img_path.suffix}\"\n",
    "                dest_path = demo_dir / dest_name\n",
    "                shutil.copy(img_path, dest_path)\n",
    "                print(f\" Added: {dest_name} → {best_breed} ({best_conf:.2f})\")\n",
    "                demo_saved += 1\n",
    "                breed_count[breed] += 1\n",
    "                if demo_saved >= 10:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\" Error processing {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\n Demo set ready: {demo_saved} images in {demo_dir}\")\n",
    "print(\" Breed distribution:\")\n",
    "for breed, count in breed_count.items():\n",
    "    if count > 0:\n",
    "        print(f\"  {breed}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75493732",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "predict_and_auto_label(\"/content/demo_images/Kankrej_7_95pct.jpg\", 0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad5816c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "retrain_with_new_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79038921",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!ls -l \"/content/Dataset2/Train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8282e553",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for breed in os.listdir(\"/content/Dataset3/Train\"):\n",
    "    count = len(os.listdir(f\"/content/Dataset3/Train/{breed}\"))\n",
    "    print(f\"train/{breed}: {count} images\")\n",
    "\n",
    "print(\"\\n---\")\n",
    "\n",
    "for breed in os.listdir(\"/content/Dataset3/Val\"):\n",
    "    count = len(os.listdir(f\"/content/Dataset3/Val/{breed}\"))\n",
    "    print(f\"val/{breed}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7581e252",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!rm \"/content/Dataset3.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b2bbd1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EfficientNet-B0 - 85 %+ val, high-confidence, calibrated\n",
    "- Soft-labels ready for pseudo-labelling\n",
    "- Temperature scaling inside TTA\n",
    "\"\"\"\n",
    "import os, json, time, torch, torchvision, numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from timm.data.auto_augment import rand_augment_transform\n",
    "from timm.data.mixup import Mixup\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "# ---------- paths ----------\n",
    "train_dir   = \"/content/New_data_train_val/train\"\n",
    "val_dir     = \"/content/New_data_train_val/val\"\n",
    "SAVE_BEST   = \"/content/best_model.pth\"\n",
    "SAVE_FINAL  = \"/content/breed_model_efficientnet_finetuned.pth\"\n",
    "SAVE_LABELS = \"/content/breed_names.json\"\n",
    "TEST_IMAGE  = \"/content/New_data_train_val/val/Gir/Gir_168.jpg\"\n",
    "\n",
    "mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "\n",
    "# ---------- augmentation ----------\n",
    "rand_aug = rand_augment_transform('rand-m7-mstd0.5', {})\n",
    "train_tf = T.Compose([T.Resize((224, 224)), rand_aug, T.ToTensor(), T.Normalize(mean, std)])\n",
    "\n",
    "# ---------- test-time 10-crop ----------\n",
    "val_tf = T.Compose([\n",
    "    T.Resize(342),\n",
    "    T.FiveCrop(256),\n",
    "    T.Lambda(lambda crops: torch.stack([T.ToTensor()(c) for c in crops])),  # 5×3×256×256\n",
    "    T.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# ---------- datasets ----------\n",
    "train_ds = datasets.ImageFolder(train_dir, transform=train_tf)\n",
    "val_ds   = datasets.ImageFolder(val_dir,   transform=val_tf)\n",
    "breed_names = train_ds.classes\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=4, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# ---------- milder mix-up ----------\n",
    "mixup_fn = Mixup(mixup_alpha=0.1, cutmix_alpha=0.2, prob=1.0,\n",
    "                 num_classes=len(breed_names), label_smoothing=0.05)\n",
    "\n",
    "# ---------- model ----------\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, in_features, n_classes, drop=0.2):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.fc   = nn.Linear(in_features, n_classes)\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.drop(x))\n",
    "\n",
    "model = models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
    "model.classifier = Head(model.classifier[1].in_features, len(breed_names))\n",
    "model = model.to(device)\n",
    "\n",
    "# ---------- freeze ----------\n",
    "for p in model.parameters(): p.requires_grad = False\n",
    "for p in model.classifier.parameters(): p.requires_grad = True\n",
    "\n",
    "# ---------- class weights (used only first 10 epochs) ----------\n",
    "labels = train_ds.targets\n",
    "weights = torch.tensor(\n",
    "    compute_class_weight('balanced', classes=np.arange(len(breed_names)), y=labels),\n",
    "    dtype=torch.float, device=device)\n",
    "\n",
    "# ---------- optim ----------\n",
    "optimizer = optim.AdamW(model.classifier.parameters(), lr=3e-3, weight_decay=5e-3)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer, T_0=len(train_loader)*5, T_mult=1, eta_min=1e-6)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def unfreeze_and_add(stage, lr):\n",
    "    frozen = []\n",
    "    for name, module in model.features.named_children():\n",
    "        if int(name) >= stage:\n",
    "            for p in module.parameters():\n",
    "                if not p.requires_grad:\n",
    "                    p.requires_grad = True\n",
    "                    frozen.append(p)\n",
    "    if frozen:\n",
    "        optimizer.add_param_group({'params': frozen, 'lr': lr})\n",
    "\n",
    "# ---------- metrics ----------\n",
    "best_val_loss = 1e9\n",
    "patience = 15\n",
    "pat_counter = 0\n",
    "accum = 2\n",
    "grad_clip = 1.0\n",
    "TEMP = 1.5  # temperature for calibration (tuned on val)\n",
    "\n",
    "# ---------- train ----------\n",
    "for epoch in range(30):\n",
    "    print(f\"\\n----- epoch {epoch+1}/30 -----\")\n",
    "    # ---- unfreeze ----\n",
    "    if epoch == 3: unfreeze_and_add(6, 3e-4)\n",
    "    if epoch == 6: unfreeze_and_add(4, 1e-4)\n",
    "    if epoch == 9: unfreeze_and_add(0, 5e-5)\n",
    "\n",
    "    # ---- loss choice ----\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights if epoch < 10 else None, label_smoothing=0.05)\n",
    "\n",
    "    # ---- train ----\n",
    "    model.train()\n",
    "    running = 0.\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        x, y = mixup_fn(x, y)\n",
    "        with autocast():\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y) / accum\n",
    "        scaler.scale(loss).backward()\n",
    "        if (i+1) % accum == 0 or i+1 == len(train_loader):\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            scaler.step(optimizer); scaler.update(); optimizer.zero_grad(set_to_none=True)\n",
    "        running += loss.item() * accum\n",
    "    print(f\"train loss {running/len(train_loader):.4f}\")\n",
    "\n",
    "    # ---- validate (10-crop) ----\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0., 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            B = y.size(0)\n",
    "            x = x.view(-1, 3, 256, 256).to(device)  # 5*crops\n",
    "            y = y.to(device)\n",
    "            with autocast():\n",
    "                out = model(x) / TEMP               # temperature scaling\n",
    "                out = out.view(5, B, -1).mean(0)    # 5-crop average\n",
    "                val_loss += criterion(out, y).item()\n",
    "            # horizontal-flip TTA\n",
    "            out_flip = model(torch.flip(x, dims=[-1])) / TEMP\n",
    "            out_flip = out_flip.view(5, B, -1).mean(0)\n",
    "            out = (out + out_flip) / 2\n",
    "            pred = out.argmax(1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += B\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = 100 * correct / total\n",
    "    print(f\"val_loss {val_loss:.4f}  val_acc {val_acc:.2f}%\")\n",
    "\n",
    "    # ---- early stop ----\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), SAVE_BEST)\n",
    "        pat_counter = 0\n",
    "    else:\n",
    "        pat_counter += 1\n",
    "        if pat_counter >= patience:\n",
    "            print(\"early stop\"); break\n",
    "    scheduler.step()\n",
    "\n",
    "# ---------- save ----------\n",
    "model.load_state_dict(torch.load(SAVE_BEST))\n",
    "torch.save(model, SAVE_FINAL)\n",
    "with open(SAVE_LABELS, \"w\") as f:\n",
    "    json.dump(breed_names, f)\n",
    "print(\"Saved best model & labels\")\n",
    "\n",
    "# ---------- high-confidence TTA ----------\n",
    "def tta_predict(image_path, temp=TEMP):\n",
    "    model.eval()\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    crops = val_tf(img).to(device)          # 5×3×256×256\n",
    "    with torch.no_grad():\n",
    "        with autocast():\n",
    "            logits = model(crops) / temp    # temperature\n",
    "            prob = torch.softmax(logits, dim=1).mean(0)\n",
    "            # flip\n",
    "            logits_flip = model(torch.flip(crops, dims=[-1])) / temp\n",
    "            prob_flip = torch.softmax(logits_flip, dim=1).mean(0)\n",
    "            prob = (prob + prob_flip) / 2\n",
    "    conf, pred = prob.max(0)\n",
    "    return breed_names[pred.item()], conf.item()\n",
    "\n",
    "if os.path.exists(TEST_IMAGE):\n",
    "    breed, conf = tta_predict(TEST_IMAGE)\n",
    "    print(f\"TTA pred: {breed}  (confidence {conf:.4f})\")\n",
    "else:\n",
    "    print(\"Test image not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c444554",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc86a68d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os, json, time, torch, torchvision, numpy as np, shutil, pandas as pd\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, models\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torch.amp import autocast, GradScaler\n",
    "from timm.data.auto_augment import rand_augment_transform\n",
    "from timm.data.mixup import Mixup\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\" Using {device}\")\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "train_dir      = \"/content/drive/MyDrive/New_data_train_val/train\"\n",
    "val_dir        = \"/content/drive/MyDrive/New_data_train_val/val\"\n",
    "UNLABELLED_DIR = \"/content/unlabelled_images\"\n",
    "PSEUDO_CSV     = \"/content/pseudo_labels.csv\"\n",
    "SAVE_BEST      = \"/content/best_model.pth\"\n",
    "SAVE_FINAL     = \"/content/breed_model_efficientnet_finetuned.pth\"\n",
    "SAVE_LABELS    = \"/content/breed_names.json\"\n",
    "TEST_IMAGE     = \"/content/drive/MyDrive/New_data_train_val/val/Kankrej/Kankrej_140.jpg\"\n",
    "\n",
    "SOFT_THR       = 0.90\n",
    "TEMP           = 1.2\n",
    "RETRAIN_EPOCHS = 5\n",
    "mean, std      = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "\n",
    "# ---------- transforms ----------\n",
    "rand_aug = rand_augment_transform('rand-m7-mstd0.5', {})\n",
    "train_tf = T.Compose([T.Resize((224, 224)), rand_aug, T.ToTensor(), T.Normalize(mean, std)])\n",
    "unlabelled_tf = T.Compose([\n",
    "    T.Resize(342), T.FiveCrop(256),\n",
    "    T.Lambda(lambda crops: torch.stack([T.ToTensor()(c) for c in crops])),\n",
    "    T.Normalize(mean, std)\n",
    "])\n",
    "val_tf = T.Compose([\n",
    "    T.Resize(342), T.FiveCrop(256),\n",
    "    T.Lambda(lambda crops: torch.stack([T.ToTensor()(c) for c in crops])),\n",
    "    T.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# ---------- datasets ----------\n",
    "train_ds = datasets.ImageFolder(train_dir, transform=train_tf)\n",
    "val_ds   = datasets.ImageFolder(val_dir,   transform=val_tf)\n",
    "breed_names = train_ds.classes\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=4, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# ---------- mixup ----------\n",
    "mixup_fn = Mixup(mixup_alpha=0.1, cutmix_alpha=0.2, prob=1.0,\n",
    "                 num_classes=len(breed_names), label_smoothing=0.05)\n",
    "\n",
    "# ---------- model ----------\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, in_features, n_classes, drop=0.2):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.fc   = nn.Linear(in_features, n_classes)\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.drop(x))\n",
    "\n",
    "model = models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
    "model.classifier = Head(model.classifier[1].in_features, len(breed_names))\n",
    "model = model.to(device)\n",
    "\n",
    "# ---------- freeze ----------\n",
    "for p in model.parameters(): p.requires_grad = False\n",
    "for p in model.classifier.parameters(): p.requires_grad = True\n",
    "\n",
    "# ---------- class weights ----------\n",
    "labels = train_ds.targets\n",
    "weights = torch.tensor(\n",
    "    compute_class_weight('balanced', classes=np.arange(len(breed_names)), y=labels),\n",
    "    dtype=torch.float, device=device)\n",
    "\n",
    "# ---------- optim ----------\n",
    "optimizer = optim.AdamW(model.classifier.parameters(), lr=3e-3, weight_decay=5e-3)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer, T_0=len(train_loader)*5, T_mult=1, eta_min=1e-6)\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def unfreeze_and_add(stage, lr):\n",
    "    frozen = []\n",
    "    for name, module in model.features.named_children():\n",
    "        if int(name) >= stage:\n",
    "            for p in module.parameters():\n",
    "                if not p.requires_grad:\n",
    "                    p.requires_grad = True\n",
    "                    frozen.append(p)\n",
    "    if frozen:\n",
    "        optimizer.add_param_group({'params': frozen, 'lr': lr})\n",
    "\n",
    "# ---------- global metrics ----------\n",
    "best_val_loss = 1e9\n",
    "patience = 15\n",
    "pat_counter = 0\n",
    "accum = 2\n",
    "grad_clip = 1.0\n",
    "TEMP = 1.2\n",
    "\n",
    "# ===================================================================\n",
    "#  1. MAIN TRAINING\n",
    "# ===================================================================\n",
    "def main_training():\n",
    "    global best_val_loss, pat_counter\n",
    "    for epoch in range(30):\n",
    "        print(f\"\\n----- 🐄 Epoch {epoch+1}/30 -----\")\n",
    "        if epoch == 3: unfreeze_and_add(6, 3e-4)\n",
    "        if epoch == 6: unfreeze_and_add(4, 1e-4)\n",
    "        if epoch == 9: unfreeze_and_add(0, 5e-5)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights if epoch < 10 else None, label_smoothing=0.05)\n",
    "\n",
    "        model.train()\n",
    "        running = 0.\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            x, y = mixup_fn(x, y)\n",
    "            with autocast('cuda'):\n",
    "                out = model(x)\n",
    "                loss = criterion(out, y) / accum\n",
    "            scaler.scale(loss).backward()\n",
    "            if (i+1) % accum == 0 or i+1 == len(train_loader):\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                scaler.step(optimizer); scaler.update(); optimizer.zero_grad(set_to_none=True)\n",
    "            running += loss.item() * accum\n",
    "        print(f\"📈 Train Loss: {running/len(train_loader):.4f}\")\n",
    "\n",
    "        # ---- validate ----\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0., 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                B = y.size(0)\n",
    "                x = x.view(-1, 3, 256, 256).to(device)\n",
    "                y = y.to(device)\n",
    "                with autocast('cuda'):\n",
    "                    out = model(x) / TEMP\n",
    "                    out = out.view(5, B, -1).mean(0)\n",
    "                    out_flip = model(torch.flip(x, dims=[-1])) / TEMP\n",
    "                    out_flip = out_flip.view(5, B, -1).mean(0)\n",
    "                    out = (out + out_flip) / 2\n",
    "                    val_loss += nn.CrossEntropyLoss()(out, y).item()\n",
    "                pred = out.argmax(1)\n",
    "                correct += (pred == y).sum().item()\n",
    "                total += B\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100 * correct / total\n",
    "        print(f\" Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), SAVE_BEST)\n",
    "            pat_counter = 0\n",
    "            print(f\" New best model saved (val_loss={val_loss:.4f})\")\n",
    "        else:\n",
    "            pat_counter += 1\n",
    "            if pat_counter >= patience:\n",
    "                print(\" Early stopping triggered\")\n",
    "                break\n",
    "        scheduler.step()\n",
    "\n",
    "    model.load_state_dict(torch.load(SAVE_BEST))\n",
    "    torch.save(model, SAVE_FINAL)\n",
    "    with open(SAVE_LABELS, \"w\") as f:\n",
    "        json.dump(breed_names, f)\n",
    "    print(\" Main training finished – best model saved\")\n",
    "\n",
    "# ===================================================================\n",
    "#  2. PSEUDO + HUMAN-IN-THE-LOOP\n",
    "# ===================================================================\n",
    "def predict_image(image_path, temp=TEMP):\n",
    "    model.eval()\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    crops = unlabelled_tf(img).to(device)\n",
    "    with torch.no_grad():\n",
    "        with autocast('cuda'):\n",
    "            logits = model(crops) / temp\n",
    "            prob = torch.softmax(logits, dim=1).mean(0)\n",
    "            logits_flip = model(torch.flip(crops, dims=[-1])) / temp\n",
    "            prob_flip = torch.softmax(logits_flip, dim=1).mean(0)\n",
    "            prob = (prob + prob_flip) / 2\n",
    "    top3 = torch.topk(prob, k=3)\n",
    "    return [(breed_names[i], float(p)) for i, p in zip(top3.indices, top3.values)], prob.cpu()\n",
    "\n",
    "def worker_choice(image_path, top3):\n",
    "    print(f\"\\n Image: {image_path}\")\n",
    "    for idx, (b, p) in enumerate(top3, 1):\n",
    "        print(f\"  {idx}. {b}  ({p*100:.1f}%)\")\n",
    "    while True:\n",
    "        try:\n",
    "            choice = input(\" Choose 1-3 (or 0 to skip): \").strip()\n",
    "            if choice == \"0\":\n",
    "                return None\n",
    "            choice = int(choice)\n",
    "            if 1 <= choice <= 3:\n",
    "                return top3[choice-1][0]\n",
    "            else:\n",
    "                print(\" Please enter 0, 1, 2, or 3.\")\n",
    "        except ValueError:\n",
    "            print(\" Invalid input – please enter a number.\")\n",
    "\n",
    "def create_pseudo_csv():\n",
    "    unlabelled_path = Path(UNLABELLED_DIR)\n",
    "    if not unlabelled_path.exists():\n",
    "        print(f\" Unlabelled directory not found: {UNLABELLED_DIR}\")\n",
    "        pd.DataFrame(columns=['file', 'label', 'confidence', 'type']).to_csv(PSEUDO_CSV, index=False)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Find image files\n",
    "    image_files = list(unlabelled_path.rglob(\"*.[jJ][pP][gG]\")) + \\\n",
    "                  list(unlabelled_path.rglob(\"*.[jJ][pP][eE][gG]\")) + \\\n",
    "                  list(unlabelled_path.rglob(\"*.[pP][nN][gG]\"))\n",
    "\n",
    "    if len(image_files) == 0:\n",
    "        print(f\" No images found in {UNLABELLED_DIR}\")\n",
    "        pd.DataFrame(columns=['file', 'label', 'confidence', 'type']).to_csv(PSEUDO_CSV, index=False)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\" Found {len(image_files)} images to label...\")\n",
    "    records = []\n",
    "\n",
    "    for img_path in image_files:\n",
    "        try:\n",
    "            top3, soft_prob = predict_image(img_path)\n",
    "            best_breed, best_conf = top3[0]\n",
    "\n",
    "            if best_conf >= SOFT_THR:\n",
    "                records.append({\n",
    "                    'file': str(img_path),\n",
    "                    'label': best_breed,\n",
    "                    'confidence': best_conf,\n",
    "                    'type': 'pseudo'\n",
    "                })\n",
    "                print(f\" Auto-labeled: {img_path.name} → {best_breed} ({best_conf:.2f})\")\n",
    "            else:\n",
    "                chosen = worker_choice(img_path, top3)\n",
    "                if chosen is not None:\n",
    "                    records.append({\n",
    "                        'file': str(img_path),\n",
    "                        'label': chosen,\n",
    "                        'confidence': 1.0,\n",
    "                        'type': 'human'\n",
    "                    })\n",
    "                    print(f\" Human-labeled: {img_path.name} → {chosen}\")\n",
    "                else:\n",
    "                    print(f\"⏭ Skipped: {img_path.name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error processing {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(PSEUDO_CSV, index=False)\n",
    "    print(f\" CSV saved → {len(df)} new labels collected\")\n",
    "    return df\n",
    "\n",
    "# ===================================================================\n",
    "#  3. SOFT-HARD DATASET + RETRAIN\n",
    "# ===================================================================\n",
    "class SoftHardDataset(Dataset):\n",
    "    def __init__(self, csv_file, root, transform, class_to_idx):\n",
    "        if not os.path.exists(csv_file):\n",
    "            self.samples = []\n",
    "            return\n",
    "        try:\n",
    "            self.df = pd.read_csv(csv_file)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            self.df = pd.DataFrame()\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.samples = []\n",
    "        if len(self.df) > 0:\n",
    "            for _, row in self.df.iterrows():\n",
    "                self.samples.append((row['file'], row['label'], row['confidence'], row['type']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label, conf, typ = self.samples[idx]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if typ == 'pseudo':\n",
    "            target = torch.zeros(len(self.class_to_idx))\n",
    "            target[self.class_to_idx[label]] = conf\n",
    "            return img, target.float()\n",
    "        else:\n",
    "            return img, self.class_to_idx[label]\n",
    "\n",
    "def retrain_with_new_data(extra_epochs=RETRAIN_EPOCHS):\n",
    "    global accum, grad_clip\n",
    "\n",
    "    print(\"\\n Checking for pseudo-labels...\")\n",
    "\n",
    "    # Safety check: if CSV doesn't exist or is empty, skip\n",
    "    if not os.path.exists(PSEUDO_CSV):\n",
    "        print(\" Pseudo-labels CSV not found — skipping retraining.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(PSEUDO_CSV)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\" Pseudo-labels CSV is empty — skipping retraining.\")\n",
    "        return\n",
    "\n",
    "    if len(df) == 0:\n",
    "        print(\" No pseudo-labels collected — skipping retraining.\")\n",
    "        return\n",
    "\n",
    "    print(f\" Found {len(df)} pseudo/human labels — starting retraining...\")\n",
    "\n",
    "    merged_root = \"/content/merged_train\"\n",
    "    os.makedirs(merged_root, exist_ok=True)\n",
    "\n",
    "    # Copy original training data\n",
    "    os.system(f\"cp -r {train_dir}/* {merged_root}/ 2>/dev/null || echo 'Original data copied'\")\n",
    "\n",
    "    # Copy pseudo-labeled images into breed folders\n",
    "    for _, row in df.iterrows():\n",
    "        src = Path(row['file'])\n",
    "        breed = row['label']\n",
    "        breed_dir = Path(merged_root) / breed\n",
    "        breed_dir.mkdir(exist_ok=True)\n",
    "        dst = breed_dir / src.name\n",
    "        if not dst.exists():\n",
    "            shutil.copy(src, dst)\n",
    "\n",
    "    # Create dataset and loader\n",
    "    merged_ds = SoftHardDataset(PSEUDO_CSV, merged_root, transform=train_tf,\n",
    "                                class_to_idx=train_ds.class_to_idx)\n",
    "    if len(merged_ds) == 0:\n",
    "        print(\" Merged dataset is empty — skipping retraining.\")\n",
    "        return\n",
    "\n",
    "    merged_loader = DataLoader(merged_ds, batch_size=16, shuffle=True,\n",
    "                               num_workers=2, pin_memory=True)\n",
    "\n",
    "    # Add all parameters to optimizer (for fine-tuning)\n",
    "    optimizer.add_param_group({'params': model.parameters(), 'lr': 1e-4})\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                    optimizer, T_0=len(merged_loader)*3, T_mult=1, eta_min=1e-6)\n",
    "\n",
    "    # Custom criterion for soft/hard labels\n",
    "    def criterion(out, y):\n",
    "        if y.dtype is torch.float32:\n",
    "            return -torch.sum(y * torch.log_softmax(out, dim=1), dim=1).mean()\n",
    "        else:\n",
    "            return nn.CrossEntropyLoss()(out, y)\n",
    "\n",
    "    # Retrain loop\n",
    "    for epoch in range(extra_epochs):\n",
    "        print(f\"\\n+++  Pseudo Epoch {epoch+1}/{extra_epochs} +++\")\n",
    "        model.train()\n",
    "        running = 0.\n",
    "        for x, y in merged_loader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            with autocast('cuda'):\n",
    "                out = model(x)\n",
    "                loss = criterion(out, y) / accum\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            scaler.step(optimizer); scaler.update(); optimizer.zero_grad(set_to_none=True)\n",
    "            running += loss.item() * accum\n",
    "        print(f\" Merged Loss: {running/len(merged_loader):.4f}\")\n",
    "\n",
    "    torch.save(model, \"/content/model_after_pseudo.pth\")\n",
    "    print(\" Pseudo-training finished – model saved\")\n",
    "\n",
    "# ===================================================================\n",
    "#  4. RUN PIPELINE\n",
    "# ===================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\" Starting full training pipeline...\")\n",
    "    main_training()               # 1. Train on original data\n",
    "    df = create_pseudo_csv()      # 2. Collect pseudo + human labels\n",
    "    if len(df) > 0:\n",
    "        retrain_with_new_data()   # 3. Retrain on merged set (only if we have labels)\n",
    "    else:\n",
    "        print(\"⏭ Skipping retraining — no new labels collected.\")\n",
    "\n",
    "    print(\"\\n PIPELINE COMPLETE!\")\n",
    "    print(\" Final model: /content/breed_model_efficientnet_finetuned.pth\")\n",
    "    print(\"  Breed names: /content/breed_names.json\")\n",
    "    if os.path.exists(\"/content/model_after_pseudo.pth\"):\n",
    "        print(\" Upgraded model: /content/model_after_pseudo.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee6a871",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# finall improved code by :- deepseek\n",
    "import os, json, time, torch, torchvision, numpy as np, shutil, pandas as pd\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, models\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torch.amp import autocast, GradScaler\n",
    "from timm.data.auto_augment import rand_augment_transform\n",
    "from timm.data.mixup import Mixup\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\" Using {device}\")\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "train_dir      = \"/content/drive/MyDrive/New_data_train_val/train\"\n",
    "val_dir        = \"/content/drive/MyDrive/New_data_train_val/val\"\n",
    "UNLABELLED_DIR = \"/content/unlabelled_images\"\n",
    "PROCESSED_DIR = \"/content/processed_images\"  # To store images after processing\n",
    "PSEUDO_CSV     = \"/content/pseudo_labels.csv\"\n",
    "SAVE_BEST      = \"/content/best_model.pth\"\n",
    "SAVE_FINAL     = \"/content/breed_model_efficientnet_finetuned.pth\"\n",
    "SAVE_LABELS    = \"/content/breed_names.json\"\n",
    "TEST_IMAGE     = \"/content/drive/MyDrive/New_data_train_val/val/Sahiwal/Sahiwal_155.jpg\"\n",
    "\n",
    "SOFT_THR       = 0.90  # Confidence threshold for auto-labeling\n",
    "TEMP           = 1.2\n",
    "RETRAIN_EPOCHS = 5\n",
    "mean, std      = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(UNLABELLED_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- transforms ----------\n",
    "rand_aug = rand_augment_transform('rand-m7-mstd0.5', {})\n",
    "train_tf = T.Compose([T.Resize((224, 224)), rand_aug, T.ToTensor(), T.Normalize(mean, std)])\n",
    "unlabelled_tf = T.Compose([\n",
    "    T.Resize(342), T.FiveCrop(256),\n",
    "    T.Lambda(lambda crops: torch.stack([T.ToTensor()(c) for c in crops])),\n",
    "    T.Normalize(mean, std)\n",
    "])\n",
    "val_tf = T.Compose([\n",
    "    T.Resize(342), T.FiveCrop(256),\n",
    "    T.Lambda(lambda crops: torch.stack([T.ToTensor()(c) for c in crops])),\n",
    "    T.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# ---------- datasets ----------\n",
    "train_ds = datasets.ImageFolder(train_dir, transform=train_tf)\n",
    "val_ds   = datasets.ImageFolder(val_dir,   transform=val_tf)\n",
    "breed_names = train_ds.classes\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=4, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# ---------- mixup ----------\n",
    "mixup_fn = Mixup(mixup_alpha=0.1, cutmix_alpha=0.2, prob=1.0,\n",
    "                 num_classes=len(breed_names), label_smoothing=0.05)\n",
    "\n",
    "# ---------- model ----------\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, in_features, n_classes, drop=0.2):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.fc   = nn.Linear(in_features, n_classes)\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.drop(x))\n",
    "\n",
    "model = models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
    "model.classifier = Head(model.classifier[1].in_features, len(breed_names))\n",
    "model = model.to(device)\n",
    "\n",
    "# ---------- freeze ----------\n",
    "for p in model.parameters(): p.requires_grad = False\n",
    "for p in model.classifier.parameters(): p.requires_grad = True\n",
    "\n",
    "# ---------- class weights ----------\n",
    "labels = train_ds.targets\n",
    "weights = torch.tensor(\n",
    "    compute_class_weight('balanced', classes=np.arange(len(breed_names)), y=labels),\n",
    "    dtype=torch.float, device=device)\n",
    "\n",
    "# ---------- optim ----------\n",
    "optimizer = optim.AdamW(model.classifier.parameters(), lr=3e-3, weight_decay=5e-3)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer, T_0=len(train_loader)*5, T_mult=1, eta_min=1e-6)\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def unfreeze_and_add(stage, lr):\n",
    "    frozen = []\n",
    "    for name, module in model.features.named_children():\n",
    "        if int(name) >= stage:\n",
    "            for p in module.parameters():\n",
    "                if not p.requires_grad:\n",
    "                    p.requires_grad = True\n",
    "                    frozen.append(p)\n",
    "    if frozen:\n",
    "        optimizer.add_param_group({'params': frozen, 'lr': lr})\n",
    "\n",
    "# ---------- global metrics ----------\n",
    "best_val_loss = 1e9\n",
    "patience = 15\n",
    "pat_counter = 0\n",
    "accum = 2\n",
    "grad_clip = 1.0\n",
    "TEMP = 1.2\n",
    "\n",
    "# ===================================================================\n",
    "#  CAPTURE/STORE IMAGES FROM FLWs\n",
    "# ===================================================================\n",
    "def capture_and_store_image(image_path=None, image_array=None, camera_index=0):\n",
    "    \"\"\"\n",
    "    Capture an image from camera or save provided image to unlabelled folder\n",
    "    \"\"\"\n",
    "    if image_path and os.path.exists(image_path):\n",
    "        # Copy provided image to unlabelled folder\n",
    "        filename = os.path.basename(image_path)\n",
    "        dest_path = os.path.join(UNLABELLED_DIR, f\"{int(time.time())}_{filename}\")\n",
    "        shutil.copy(image_path, dest_path)\n",
    "        print(f\" Image saved to {dest_path}\")\n",
    "        return dest_path\n",
    "    elif image_array is not None:\n",
    "        # Save image array to unlabelled folder\n",
    "        filename = f\"captured_{int(time.time())}.jpg\"\n",
    "        dest_path = os.path.join(UNLABELLED_DIR, filename)\n",
    "        cv2.imwrite(dest_path, image_array)\n",
    "        print(f\" Image saved to {dest_path}\")\n",
    "        return dest_path\n",
    "    else:\n",
    "        # Capture from camera\n",
    "        cap = cv2.VideoCapture(camera_index)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "\n",
    "        if ret:\n",
    "            filename = f\"captured_{int(time.time())}.jpg\"\n",
    "            dest_path = os.path.join(UNLABELLED_DIR, filename)\n",
    "            cv2.imwrite(dest_path, frame)\n",
    "            print(f\" Image captured and saved to {dest_path}\")\n",
    "            return dest_path\n",
    "        else:\n",
    "            print(\" Failed to capture image from camera\")\n",
    "            return None\n",
    "\n",
    "# ===================================================================\n",
    "#  1. MAIN TRAINING\n",
    "# ===================================================================\n",
    "def main_training():\n",
    "    global best_val_loss, pat_counter\n",
    "    for epoch in range(30):\n",
    "        print(f\"\\n-----  Epoch {epoch+1}/30 -----\")\n",
    "        if epoch == 3: unfreeze_and_add(6, 3e-4)\n",
    "        if epoch == 6: unfreeze_and_add(4, 1e-4)\n",
    "        if epoch == 9: unfreeze_and_add(0, 5e-5)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights if epoch < 10 else None, label_smoothing=0.05)\n",
    "\n",
    "        model.train()\n",
    "        running = 0.\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            x, y = mixup_fn(x, y)\n",
    "            with autocast('cuda'):\n",
    "                out = model(x)\n",
    "                loss = criterion(out, y) / accum\n",
    "            scaler.scale(loss).backward()\n",
    "            if (i+1) % accum == 0 or i+1 == len(train_loader):\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                scaler.step(optimizer); scaler.update(); optimizer.zero_grad(set_to_none=True)\n",
    "            running += loss.item() * accum\n",
    "        print(f\" Train Loss: {running/len(train_loader):.4f}\")\n",
    "\n",
    "        # ---- validate ----\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0., 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                B = y.size(0)\n",
    "                x = x.view(-1, 3, 256, 256).to(device)\n",
    "                y = y.to(device)\n",
    "                with autocast('cuda'):\n",
    "                    out = model(x) / TEMP\n",
    "                    out = out.view(5, B, -1).mean(0)\n",
    "                    out_flip = model(torch.flip(x, dims=[-1])) / TEMP\n",
    "                    out_flip = out_flip.view(5, B, -1).mean(0)\n",
    "                    out = (out + out_flip) / 2\n",
    "                    val_loss += nn.CrossEntropyLoss()(out, y).item()\n",
    "                pred = out.argmax(1)\n",
    "                correct += (pred == y).sum().item()\n",
    "                total += B\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100 * correct / total\n",
    "        print(f\" Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), SAVE_BEST)\n",
    "            pat_counter = 0\n",
    "            print(f\" New best model saved (val_loss={val_loss:.4f})\")\n",
    "        else:\n",
    "            pat_counter += 1\n",
    "            if pat_counter >= patience:\n",
    "                print(\" Early stopping triggered\")\n",
    "                break\n",
    "        scheduler.step()\n",
    "\n",
    "    model.load_state_dict(torch.load(SAVE_BEST))\n",
    "    torch.save(model, SAVE_FINAL)\n",
    "    with open(SAVE_LABELS, \"w\") as f:\n",
    "        json.dump(breed_names, f)\n",
    "    print(\"✅ Main training finished – best model saved\")\n",
    "\n",
    "# ===================================================================\n",
    "#  2. PSEUDO + HUMAN-IN-THE-LOOP\n",
    "# ===================================================================\n",
    "def predict_image(image_path, temp=TEMP):\n",
    "    model.eval()\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    crops = unlabelled_tf(img).to(device)\n",
    "    with torch.no_grad():\n",
    "        with autocast('cuda'):\n",
    "            logits = model(crops) / temp\n",
    "            prob = torch.softmax(logits, dim=1).mean(0)\n",
    "            logits_flip = model(torch.flip(crops, dims=[-1])) / temp\n",
    "            prob_flip = torch.softmax(logits_flip, dim=1).mean(0)\n",
    "            prob = (prob + prob_flip) / 2\n",
    "    top3 = torch.topk(prob, k=3)\n",
    "    return [(breed_names[i], float(p)) for i, p in zip(top3.indices, top3.values)], prob.cpu()\n",
    "\n",
    "def worker_choice(image_path, top3):\n",
    "    print(f\"\\n Image: {os.path.basename(image_path)}\")\n",
    "    for idx, (b, p) in enumerate(top3, 1):\n",
    "        print(f\"  {idx}. {b}  ({p*100:.1f}%)\")\n",
    "    print(\"  0. Skip this image\")\n",
    "    print(\"  4. None of the above\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            choice = input(\" Choose 0-4: \").strip()\n",
    "            if choice == \"0\":\n",
    "                return \"skip\"\n",
    "            elif choice == \"4\":\n",
    "                return \"none\"\n",
    "            choice = int(choice)\n",
    "            if 1 <= choice <= 3:\n",
    "                return top3[choice-1][0]\n",
    "            else:\n",
    "                print(\" Please enter 0, 1, 2, 3, or 4.\")\n",
    "        except ValueError:\n",
    "            print(\" Invalid input – please enter a number.\")\n",
    "\n",
    "def create_pseudo_csv():\n",
    "    unlabelled_path = Path(UNLABELLED_DIR)\n",
    "    if not unlabelled_path.exists():\n",
    "        print(f\" Unlabelled directory not found: {UNLABELLED_DIR}\")\n",
    "        pd.DataFrame(columns=['file', 'label', 'confidence', 'type', 'date']).to_csv(PSEUDO_CSV, index=False)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Find image files\n",
    "    image_files = list(unlabelled_path.rglob(\"*.[jJ][pP][gG]\")) + \\\n",
    "                  list(unlabelled_path.rglob(\"*.[jJ][pP][eE][gG]\")) + \\\n",
    "                  list(unlabelled_path.rglob(\"*.[pP][nN][gG]\"))\n",
    "\n",
    "    if len(image_files) == 0:\n",
    "        print(f\" No images found in {UNLABELLED_DIR}\")\n",
    "        pd.DataFrame(columns=['file', 'label', 'confidence', 'type', 'date']).to_csv(PSEUDO_CSV, index=False)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\" Found {len(image_files)} images to label...\")\n",
    "    records = []\n",
    "\n",
    "    # Load existing labels (if any) to avoid reprocessing\n",
    "    existing_files = set()\n",
    "    if os.path.exists(PSEUDO_CSV):\n",
    "        try:\n",
    "            existing_df = pd.read_csv(PSEUDO_CSV)\n",
    "            existing_files = set(existing_df['file'].tolist())\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for img_path in image_files:\n",
    "        # Skip if already labeled\n",
    "        if str(img_path) in existing_files:\n",
    "            print(f\"⏭ Already labeled: {img_path.name}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            top3, soft_prob = predict_image(img_path)\n",
    "            best_breed, best_conf = top3[0]\n",
    "\n",
    "            if best_conf >= SOFT_THR:\n",
    "                # Auto-label with high confidence\n",
    "                records.append({\n",
    "                    'file': str(img_path),\n",
    "                    'label': best_breed,\n",
    "                    'confidence': best_conf,\n",
    "                    'type': 'pseudo',\n",
    "                    'date': datetime.now().isoformat()\n",
    "                })\n",
    "                print(f\" Auto-labeled: {img_path.name} → {best_breed} ({best_conf:.2f})\")\n",
    "\n",
    "                # Move to processed folder\n",
    "                processed_path = os.path.join(PROCESSED_DIR, img_path.name)\n",
    "                shutil.move(str(img_path), processed_path)\n",
    "\n",
    "            else:\n",
    "                # Get human input for low confidence predictions\n",
    "                chosen = worker_choice(img_path, top3)\n",
    "\n",
    "                if chosen == \"skip\":\n",
    "                    print(f\" Skipped: {img_path.name}\")\n",
    "                    # Move to processed folder without labeling\n",
    "                    processed_path = os.path.join(PROCESSED_DIR, img_path.name)\n",
    "                    shutil.move(str(img_path), processed_path)\n",
    "\n",
    "                elif chosen == \"none\":\n",
    "                    print(f\" Manual label needed for: {img_path.name}\")\n",
    "                    manual_label = input(\" Enter the breed name manually: \").strip()\n",
    "                    if manual_label and manual_label in breed_names:\n",
    "                        records.append({\n",
    "                            'file': str(img_path),\n",
    "                            'label': manual_label,\n",
    "                            'confidence': 1.0,\n",
    "                            'type': 'human',\n",
    "                            'date': datetime.now().isoformat()\n",
    "                        })\n",
    "                        print(f\" Human-labeled: {img_path.name} → {manual_label}\")\n",
    "\n",
    "                        # Move to processed folder\n",
    "                        processed_path = os.path.join(PROCESSED_DIR, img_path.name)\n",
    "                        shutil.move(str(img_path), processed_path)\n",
    "                    else:\n",
    "                        print(f\" Invalid breed name. Skipping: {img_path.name}\")\n",
    "                        # Move to processed folder without labeling\n",
    "                        processed_path = os.path.join(PROCESSED_DIR, img_path.name)\n",
    "                        shutil.move(str(img_path), processed_path)\n",
    "\n",
    "                else:\n",
    "                    records.append({\n",
    "                        'file': str(img_path),\n",
    "                        'label': chosen,\n",
    "                        'confidence': 1.0,\n",
    "                        'type': 'human',\n",
    "                        'date': datetime.now().isoformat()\n",
    "                    })\n",
    "                    print(f\"👩‍🌾 Human-labeled: {img_path.name} → {chosen}\")\n",
    "\n",
    "                    # Move to processed folder\n",
    "                    processed_path = os.path.join(PROCESSED_DIR, img_path.name)\n",
    "                    shutil.move(str(img_path), processed_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error processing {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Merge with existing CSV\n",
    "    if os.path.exists(PSEUDO_CSV) and len(records) > 0:\n",
    "        try:\n",
    "            existing_df = pd.read_csv(PSEUDO_CSV)\n",
    "            new_df = pd.DataFrame(records)\n",
    "            df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "        except:\n",
    "            df = pd.DataFrame(records)\n",
    "    else:\n",
    "        df = pd.DataFrame(records)\n",
    "\n",
    "    df.to_csv(PSEUDO_CSV, index=False)\n",
    "    print(f\" CSV saved → {len(df)} total labels collected\")\n",
    "    return df\n",
    "\n",
    "# ===================================================================\n",
    "#  3. SOFT-HARD DATASET + RETRAIN\n",
    "# ===================================================================\n",
    "class SoftHardDataset(Dataset):\n",
    "    def __init__(self, csv_file, root, transform, class_to_idx):\n",
    "        if not os.path.exists(csv_file):\n",
    "            self.samples = []\n",
    "            return\n",
    "        try:\n",
    "            self.df = pd.read_csv(csv_file)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            self.df = pd.DataFrame()\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.samples = []\n",
    "        if len(self.df) > 0:\n",
    "            for _, row in self.df.iterrows():\n",
    "                # Check if file exists (it might have been moved to processed folder)\n",
    "                file_path = row['file']\n",
    "                if not os.path.exists(file_path):\n",
    "                    # Try to find it in the processed folder\n",
    "                    processed_path = os.path.join(PROCESSED_DIR, os.path.basename(file_path))\n",
    "                    if os.path.exists(processed_path):\n",
    "                        file_path = processed_path\n",
    "\n",
    "                self.samples.append((file_path, row['label'], row['confidence'], row['type']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label, conf, typ = self.samples[idx]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if typ == 'pseudo':\n",
    "            target = torch.zeros(len(self.class_to_idx))\n",
    "            target[self.class_to_idx[label]] = conf\n",
    "            return img, target.float()\n",
    "        else:\n",
    "            return img, self.class_to_idx[label]\n",
    "\n",
    "def retrain_with_new_data(extra_epochs=RETRAIN_EPOCHS):\n",
    "    global accum, grad_clip\n",
    "\n",
    "    print(\"\\n Checking for pseudo-labels...\")\n",
    "\n",
    "    # Safety check: if CSV doesn't exist or is empty, skip\n",
    "    if not os.path.exists(PSEUDO_CSV):\n",
    "        print(\" Pseudo-labels CSV not found — skipping retraining.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(PSEUDO_CSV)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\" Pseudo-labels CSV is empty — skipping retraining.\")\n",
    "        return\n",
    "\n",
    "    if len(df) == 0:\n",
    "        print(\" No pseudo-labels collected — skipping retraining.\")\n",
    "        return\n",
    "\n",
    "    print(f\" Found {len(df)} pseudo/human labels — starting retraining...\")\n",
    "\n",
    "    merged_root = \"/content/merged_train\"\n",
    "    os.makedirs(merged_root, exist_ok=True)\n",
    "\n",
    "    # Copy original training data\n",
    "    os.system(f\"cp -r {train_dir}/* {merged_root}/ 2>/dev/null || echo 'Original data copied'\")\n",
    "\n",
    "    # Copy pseudo-labeled images into breed folders\n",
    "    for _, row in df.iterrows():\n",
    "        # Check if file exists in original location or processed folder\n",
    "        src_path = row['file']\n",
    "        if not os.path.exists(src_path):\n",
    "            processed_path = os.path.join(PROCESSED_DIR, os.path.basename(src_path))\n",
    "            if os.path.exists(processed_path):\n",
    "                src_path = processed_path\n",
    "            else:\n",
    "                print(f\" Could not find image: {src_path}\")\n",
    "                continue\n",
    "\n",
    "        breed = row['label']\n",
    "        breed_dir = Path(merged_root) / breed\n",
    "        breed_dir.mkdir(exist_ok=True)\n",
    "        dst = breed_dir / os.path.basename(src_path)\n",
    "        if not dst.exists():\n",
    "            shutil.copy(src_path, dst)\n",
    "\n",
    "    # Create dataset and loader\n",
    "    merged_ds = SoftHardDataset(PSEUDO_CSV, merged_root, transform=train_tf,\n",
    "                                class_to_idx=train_ds.class_to_idx)\n",
    "    if len(merged_ds) == 0:\n",
    "        print(\" Merged dataset is empty — skipping retraining.\")\n",
    "        return\n",
    "\n",
    "    merged_loader = DataLoader(merged_ds, batch_size=16, shuffle=True,\n",
    "                               num_workers=2, pin_memory=True)\n",
    "\n",
    "    # Add all parameters to optimizer (for fine-tuning)\n",
    "    optimizer.add_param_group({'params': model.parameters(), 'lr': 1e-4})\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                    optimizer, T_0=len(merged_loader)*3, T_mult=1, eta_min=1e-6)\n",
    "\n",
    "    # Custom criterion for soft/hard labels\n",
    "    def criterion(out, y):\n",
    "        if y.dtype is torch.float32:\n",
    "            return -torch.sum(y * torch.log_softmax(out, dim=1), dim=1).mean()\n",
    "        else:\n",
    "            return nn.CrossEntropyLoss()(out, y)\n",
    "\n",
    "    # Retrain loop\n",
    "    for epoch in range(extra_epochs):\n",
    "        print(f\"\\n+++  Pseudo Epoch {epoch+1}/{extra_epochs} +++\")\n",
    "        model.train()\n",
    "        running = 0.\n",
    "        for x, y in merged_loader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            with autocast('cuda'):\n",
    "                out = model(x)\n",
    "                loss = criterion(out, y) / accum\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            scaler.step(optimizer); scaler.update(); optimizer.zero_grad(set_to_none=True)\n",
    "            running += loss.item() * accum\n",
    "        print(f\" Merged Loss: {running/len(merged_loader):.4f}\")\n",
    "\n",
    "    torch.save(model, \"/content/model_after_pseudo.pth\")\n",
    "    print(\" Pseudo-training finished – model saved\")\n",
    "\n",
    "# ===================================================================\n",
    "#  4. HELPER: LABEL NEW IMAGES ONLY (NO RETRAIN)\n",
    "# ===================================================================\n",
    "def label_new_images_only():\n",
    "    \"\"\"Run ONLY the labeling step — useful when adding new test photos\"\"\"\n",
    "    print(\" Starting NEW IMAGE LABELING only...\")\n",
    "\n",
    "    # Load model if not already loaded\n",
    "    global model, breed_names\n",
    "    if 'model' not in globals() or model is None:\n",
    "        print(\"Loading model...\")\n",
    "        model = torch.load(SAVE_FINAL)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        with open(SAVE_LABELS, \"r\") as f:\n",
    "            breed_names = json.load(f)\n",
    "        print(\"Model loaded.\")\n",
    "\n",
    "    df = create_pseudo_csv()  # This will label only NEW images\n",
    "    print(f\" Done labeling. Total labeled images: {len(df) if not df.empty else 0}\")\n",
    "    return df\n",
    "\n",
    "# ===================================================================\n",
    "#  5. EVALUATE SINGLE IMAGE\n",
    "# ===================================================================\n",
    "def evaluate_single_image(image_path=None, image_array=None, camera_index=0):\n",
    "    \"\"\"\n",
    "    Evaluate a single image with the model\n",
    "    Returns: prediction, confidence, top choices\n",
    "    \"\"\"\n",
    "    # Capture/store image if needed\n",
    "    if image_path or image_array is not None or camera_index >= 0:\n",
    "        stored_path = capture_and_store_image(image_path, image_array, camera_index)\n",
    "        if stored_path:\n",
    "            image_path = stored_path\n",
    "\n",
    "    if not image_path or not os.path.exists(image_path):\n",
    "        print(\" No valid image provided\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Make prediction\n",
    "    top3, probs = predict_image(image_path)\n",
    "    prediction, confidence = top3[0]\n",
    "\n",
    "    print(f\"\\n Prediction for {os.path.basename(image_path)}:\")\n",
    "    print(f\"   Breed: {prediction} (Confidence: {confidence:.2%})\")\n",
    "    print(\"\\n Top choices:\")\n",
    "    for i, (breed, conf) in enumerate(top3, 1):\n",
    "        print(f\"   {i}. {breed}: {conf:.2%}\")\n",
    "\n",
    "    return prediction, confidence, top3\n",
    "\n",
    "# ===================================================================\n",
    "#  6. RUN PIPELINE\n",
    "# ===================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\" Starting full training pipeline...\")\n",
    "    main_training()               # 1. Train on original data\n",
    "    df = create_pseudo_csv()      # 2. Collect pseudo + human labels\n",
    "    if len(df) > 0:\n",
    "        retrain_with_new_data()   # 3. Retrain on merged set (only if we have labels)\n",
    "    else:\n",
    "        print(\" Skipping retraining — no new labels collected.\")\n",
    "\n",
    "    print(\"\\n PIPELINE COMPLETE!\")\n",
    "    print(\" Final model: /content/breed_model_efficientnet_finetuned.pth\")\n",
    "    print(\"  Breed names: /content/breed_names.json\")\n",
    "    if os.path.exists(\"/content/model_after_pseudo.pth\"):\n",
    "        print(\" Upgraded model: /content/model_after_pseudo.pth\")\n",
    "\n",
    "    print(\"\\n TIP: To label NEW images later, run:\")\n",
    "    print(\"   df = label_new_images_only()\")\n",
    "    print(\"   if len(df) > 0:\")\n",
    "    print(\"       retrain_with_new_data()\")\n",
    "\n",
    "    print(\"\\n To evaluate a single image, run:\")\n",
    "    print(\"   evaluate_single_image(image_path='path/to/image.jpg')\")\n",
    "    print(\"   evaluate_single_image(camera_index=0)  # Capture from camera\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd4c1c2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9222fc72",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
